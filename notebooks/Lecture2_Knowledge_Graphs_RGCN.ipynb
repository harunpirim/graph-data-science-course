{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936beb70",
   "metadata": {},
   "source": [
    "# Lecture 2: Knowledge Graphs & R-GCN for Link Prediction (PyTorch Geometric)\n",
    "\n",
    "This notebook builds on Lecture 1 and focuses on **knowledge graphs (KGs)** and **relational graph convolutional networks (R-GCN)** for link prediction.\n",
    "\n",
    "**Learning goals**\n",
    "\n",
    "- Understand the knowledge graph formalism: entities, relations, triples.\n",
    "- See the connection between KGs and heterogeneous / multilayer networks.\n",
    "- Implement a toy biomedical KG in PyG format.\n",
    "- Train a simple R-GCN-based link prediction model with DistMult-style scoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4453dfa",
   "metadata": {},
   "source": [
    "## 1. Knowledge Graphs: Definition\n",
    "\n",
    "A **knowledge graph** is a directed, labeled multigraph:\n",
    "\n",
    "- One shared entity set \\(\\mathcal{E}\\)\n",
    "- A set of relations \\(\\mathcal{R}\\)\n",
    "- Facts represented as **triples** \\((h, r, t)\\):\n",
    "\n",
    "\\begin{equation}\n",
    "(h, r, t) \\in \\mathcal{E} \\times \\mathcal{R} \\times \\mathcal{E}\n",
    "\\end{equation}\n",
    "\n",
    "- \\(h\\): head entity\n",
    "- \\(r\\): relation type\n",
    "- \\(t\\): tail entity\n",
    "\n",
    "Examples:\n",
    "\n",
    "- (Aspirin, *treats*, Headache)\n",
    "- (Alan Turing, *born_in*, London)\n",
    "- (DrugM, *targets*, GeneA)\n",
    "\n",
    "From a network science point of view:\n",
    "\n",
    "- We have **one node type \"entity\"**.\n",
    "- Each relation \\(r\\) defines a **layer** with its own adjacency matrix \\(A_r\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca300ca",
   "metadata": {},
   "source": [
    "## 2. R-GCN: Relational Message Passing\n",
    "\n",
    "Relational GCN (R-GCN) extends GCN to handle many relation types. For entity \\(i\\), the layer update is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_i^{(l+1)} = \\sigma\\left(\n",
    "    \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{c_{i,r}} W_r^{(l)} \\mathbf{h}_j^{(l)}\n",
    "    + W_0^{(l)} \\mathbf{h}_i^{(l)}\n",
    "\\right),\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "- \\(\\mathcal{N}_r(i)\\) are neighbors of \\(i\\) under relation \\(r\\),\n",
    "- \\(W_r^{(l)}\\) is the weight matrix for relation \\(r\\),\n",
    "- \\(W_0^{(l)}\\) handles self-loops, and\n",
    "- \\(c_{i,r}\\) is a normalization constant.\n",
    "\n",
    "Again, each relation layer contributes its own \"signal\" to the update of \\(i\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b271749",
   "metadata": {},
   "source": [
    "## 3. Setup\n",
    "\n",
    "As before, we import `torch` and `torch_geometric`. \n",
    "\n",
    "> Installation commands are commented out; adapt to your environment if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torch_geometric -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import RGCNConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea5b15d",
   "metadata": {},
   "source": [
    "## 4. Build a Toy Biomedical Knowledge Graph\n",
    "\n",
    "We create a small KG with entities of implicit types:\n",
    "\n",
    "- Genes: `GeneA`, `GeneB`\n",
    "- Diseases: `DiseaseX`, `DiseaseY`\n",
    "- Drugs: `DrugM`, `DrugN`\n",
    "\n",
    "Relations:\n",
    "\n",
    "- `associates_with` (Gene → Disease)\n",
    "- `treats` (Drug → Disease)\n",
    "- `targets` (Drug → Gene)\n",
    "\n",
    "We will encode these as integer IDs and then as PyG tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entity and relation vocabularies\n",
    "entity2id = {\n",
    "    'GeneA': 0,\n",
    "    'GeneB': 1,\n",
    "    'DiseaseX': 2,\n",
    "    'DiseaseY': 3,\n",
    "    'DrugM': 4,\n",
    "    'DrugN': 5,\n",
    "}\n",
    "num_entities = len(entity2id)\n",
    "\n",
    "rel2id = {\n",
    "    'associates_with': 0,  # Gene - Disease\n",
    "    'treats': 1,           # Drug - Disease\n",
    "    'targets': 2,          # Drug - Gene\n",
    "}\n",
    "num_relations = len(rel2id)\n",
    "\n",
    "# Define triples (h, r, t)\n",
    "triples = [\n",
    "    ('GeneA', 'associates_with', 'DiseaseX'),\n",
    "    ('GeneB', 'associates_with', 'DiseaseY'),\n",
    "    ('DrugM', 'treats', 'DiseaseX'),\n",
    "    ('DrugN', 'treats', 'DiseaseY'),\n",
    "    ('DrugM', 'targets', 'GeneA'),\n",
    "    ('DrugN', 'targets', 'GeneB'),\n",
    "]\n",
    "\n",
    "heads = torch.tensor([entity2id[h] for (h, r, t) in triples])\n",
    "rels  = torch.tensor([rel2id[r]    for (h, r, t) in triples])\n",
    "tails = torch.tensor([entity2id[t] for (h, r, t) in triples])\n",
    "\n",
    "edge_index = torch.stack([heads, tails], dim=0)   # shape [2, num_edges]\n",
    "edge_type  = rels                                  # shape [num_edges]\n",
    "\n",
    "num_edges = edge_index.size(1)\n",
    "num_entities, num_edges, num_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31dc32d",
   "metadata": {},
   "source": [
    "## 5. Define an R-GCN Encoder\n",
    "\n",
    "We define a small R-GCN with two layers. Node features are learnable embeddings initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47270412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, emb_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # Initial entity embeddings\n",
    "        self.entity_emb = nn.Embedding(num_entities, emb_dim)\n",
    "\n",
    "        self.conv1 = RGCNConv(\n",
    "            in_channels=emb_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            num_relations=num_relations\n",
    "        )\n",
    "        self.conv2 = RGCNConv(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            num_relations=num_relations\n",
    "        )\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = self.entity_emb.weight  # [num_entities, emb_dim]\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x  # final entity embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467238a6",
   "metadata": {},
   "source": [
    "## 6. Link Prediction with DistMult-Style Scoring\n",
    "\n",
    "We use a simple DistMult-like score function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi(h, r, t) = \\langle \\mathbf{e}_h, \\mathbf{r}_r, \\mathbf{e}_t \\rangle\n",
    "= \\sum_k e_{h,k} \\, r_{r,k} \\, e_{t,k}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "- \\(\\mathbf{e}_h\\) and \\(\\mathbf{e}_t\\) are entity embeddings,\n",
    "- \\(\\mathbf{r}_r\\) is a relation embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNLinkPredictor(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, emb_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.rgcn = RGCN(num_entities, num_relations, emb_dim, hidden_dim)\n",
    "        self.rel_emb = nn.Embedding(num_relations, hidden_dim)\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        # Return entity embeddings\n",
    "        return self.rgcn(edge_index, edge_type)\n",
    "\n",
    "    def score_triples(self, entity_emb, heads, rels, tails):\n",
    "        # entity_emb: [num_entities, hidden_dim]\n",
    "        h = entity_emb[heads]           # [B, d]\n",
    "        r = self.rel_emb(rels)         # [B, d]\n",
    "        t = entity_emb[tails]         # [B, d]\n",
    "        return (h * r * t).sum(dim=-1)  # [B]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ef610",
   "metadata": {},
   "source": [
    "## 7. Negative Sampling and Training Loop\n",
    "\n",
    "For each positive triple \\((h, r, t)\\), we generate a negative triple by corrupting the tail with a random entity. We then optimize a binary cross-entropy loss over positive and negative scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bdc6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def negative_sampling(num_entities, heads, rels, tails):\n",
    "    \"\"\"Very simple negative sampler: replace tail with a random entity.\"\"\"\n",
    "    neg_tails = tails.clone()\n",
    "    for i in range(len(tails)):\n",
    "        neg_tails[i] = random.randrange(num_entities)\n",
    "    return heads, rels, neg_tails\n",
    "\n",
    "# Move tensors and model to device\n",
    "model = RGCNLinkPredictor(num_entities, num_relations).to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "edge_type = edge_type.to(device)\n",
    "heads = heads.to(device)\n",
    "rels = rels.to(device)\n",
    "tails = tails.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_link_pred(epochs=200):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1) Get entity embeddings from R-GCN\n",
    "        entity_emb = model(edge_index, edge_type)\n",
    "\n",
    "        # 2) Positive scores\n",
    "        pos_scores = model.score_triples(entity_emb, heads, rels, tails)\n",
    "\n",
    "        # 3) Negative samples & scores\n",
    "        neg_heads, neg_rels, neg_tails = negative_sampling(num_entities, heads, rels, tails)\n",
    "        neg_heads = neg_heads.to(device)\n",
    "        neg_rels  = neg_rels.to(device)\n",
    "        neg_tails = neg_tails.to(device)\n",
    "        neg_scores = model.score_triples(entity_emb, neg_heads, neg_rels, neg_tails)\n",
    "\n",
    "        # 4) Labels: 1 for pos, 0 for neg\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "        labels = torch.cat([\n",
    "            torch.ones_like(pos_scores),\n",
    "            torch.zeros_like(neg_scores)\n",
    "        ], dim=0)\n",
    "\n",
    "        loss = bce_loss(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                pos_prob = torch.sigmoid(pos_scores)\n",
    "                neg_prob = torch.sigmoid(neg_scores)\n",
    "                print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
    "                      f\"Pos prob mean: {pos_prob.mean().item():.3f}, \"\n",
    "                      f\"Neg prob mean: {neg_prob.mean().item():.3f}\")\n",
    "\n",
    "\n",
    "train_link_pred(epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ffbf9",
   "metadata": {},
   "source": [
    "## 8. Inspect Learned Embeddings and Scores\n",
    "\n",
    "We can inspect the scores for all true triples and compare them with some random negatives to see if the model has learned reasonable distinctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inspect_scores():\n",
    "    model.eval()\n",
    "    entity_emb = model(edge_index, edge_type)\n",
    "\n",
    "    pos_scores = torch.sigmoid(model.score_triples(entity_emb, heads, rels, tails))\n",
    "\n",
    "    print(\"\\nPositive triples and their scores:\")\n",
    "    for i, (h, r, t) in enumerate(triples):\n",
    "        print(f\"{(h, r, t)} -> score={pos_scores[i].item():.3f}\")\n",
    "\n",
    "    # Sample some random negative triples\n",
    "    neg_heads, neg_rels, neg_tails = negative_sampling(num_entities, heads, rels, tails)\n",
    "    neg_scores = torch.sigmoid(model.score_triples(entity_emb, neg_heads.to(device),\n",
    "                                                   neg_rels.to(device), neg_tails.to(device)))\n",
    "\n",
    "    print(\"\\nExample negative triples and their scores:\")\n",
    "    for i in range(len(triples)):\n",
    "        h_id, r_id, t_id = int(neg_heads[i]), int(neg_rels[i]), int(neg_tails[i])\n",
    "        inv_entity = {v: k for k, v in entity2id.items()}\n",
    "        inv_rel = {v: k for k, v in rel2id.items()}\n",
    "        h_name, r_name, t_name = inv_entity[h_id], inv_rel[r_id], inv_entity[t_id]\n",
    "        print(f\"{(h_name, r_name, t_name)} -> score={neg_scores[i].item():.3f}\")\n",
    "\n",
    "\n",
    "inspect_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f318b2d",
   "metadata": {},
   "source": [
    "## 9. Discussion & Exercises\n",
    "\n",
    "**Conceptual questions:**\n",
    "\n",
    "1. View each relation as a distinct **layer** in a multilayer network. How does R-GCN combine layer-specific signals?\n",
    "2. What happens if we remove one relation type (e.g., `targets`)? Can we still infer which drug treats which disease?\n",
    "\n",
    "**Coding exercises:**\n",
    "\n",
    "1. Extend the KG with an additional entity type, e.g., `VariantX`, and relation `has_variant` (Gene → Variant).\n",
    "2. Implement a small evaluation routine that computes Hits@k or Mean Reciprocal Rank (MRR) on a held-out set of triples.\n",
    "3. Compare R-GCN-based link prediction with a simple translational KGE model like TransE on this toy data.\n",
    "\n",
    "These exercises connect classical network science ideas (layers, adjacency matrices, diffusion) with modern graph ML models on heterogeneous and knowledge graphs.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}