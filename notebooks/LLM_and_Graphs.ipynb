{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMM and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aH3Q56MGDARI"
   },
   "source": [
    "Let's create a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lDLHAhIlDEvm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
      "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Assume a toy dataset with 3 papers (nodes), edges, and labels\n",
    "data = Data(\n",
    "    x=torch.rand(3, 10),  # Random node features\n",
    "    edge_index=torch.tensor([[0, 1], [1, 2]], dtype=torch.long).t().contiguous(),  # Edges (transposed for PyG)\n",
    "    y=torch.tensor([0, 1, 2], dtype=torch.long),  # True labels (3 classes)\n",
    "    text=[\"Paper A abstract about machine learning\", \n",
    "          \"Paper B abstract about deep learning\", \n",
    "          \"Paper C abstract about neural networks\"],  # Text data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "  Number of nodes: 3\n",
      "  Node feature dimension: 10\n",
      "  Number of edges: 2\n",
      "  Number of classes: 3\n",
      "  True labels: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(torch.unique(data.y))  # Number of unique classes\n",
    "\n",
    "print(f\"Dataset info:\")\n",
    "print(f\"  Number of nodes: {data.x.size(0)}\")\n",
    "print(f\"  Node feature dimension: {data.x.size(1)}\")\n",
    "print(f\"  Number of edges: {data.edge_index.size(1)}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  True labels: {data.y.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388,
     "referenced_widgets": [
      "a0f9087c1517444d9d02d3ee239871c0",
      "49409686002944ba913043f9e566dec6",
      "9ca2c22d54a14543a0ccb418f84dfc7a",
      "6ef1fa9fc050493a88aa2ba634c36cb4",
      "95dde357b3704448bfdb0b3a8818413e",
      "918e3e0abaaa43119b27a725d9996622",
      "c45bf94c8ae1497bb5093dd459e35868",
      "78a75c6a1b0444baa32a8dc0444c6e42",
      "efae36a6383c472c8d8fe7f39223a13f",
      "722fd967e7a640468b2c1c828c6ad983",
      "9b6d7d228f25418aaf2489fedc465492",
      "0b92a27e384b4ccfa1717e2898c44499",
      "694e7d436acc4f5288c6dddcb1e8eb43",
      "f8dd5aa75d5342b9be4b9ef0914b3330",
      "577d03563d0e429da232b47df6dc2cfb",
      "da4c66ea361b4ea4be05010c02866f51",
      "f500df21edf1499c83e6d0036e5b771c",
      "26329100e08f452bba6a2cb0199c84b7",
      "bcbc986b382a4ba49bfc4d5fd428488c",
      "bced6297dc0b46ea826ee40344c21b13",
      "04e572cf4bfe4c4b91b08f47cdb88c77",
      "1c795237b0964f4182a2f2948b747f94",
      "63bc10c1908945928df1d9b492912a04",
      "a87bb9d072ee43a08a97c9effb82db9c",
      "000c7c5876a04721ac8ccc89d9e6163e",
      "ccbc8df9abab48948c5317bcc31612e1",
      "87df42e6746e4847864eda694620750f",
      "a88f61637e6541f589187d5d9abcb503",
      "bb0d43a5c4f34b1d9a39230b9c129fd5",
      "5bf85ad0b72a4e68914487ffabef5ea6",
      "f8d2202f01874c30a3ae563e6fdd6428",
      "853f6e10e86f4d8aba5822790d43a34e",
      "581e482879434621bb48245bc886baa2",
      "b369bdad965b4b3794af271f5416d42a",
      "07c929ea8b0e4f6a85bfa54b60beec27",
      "e2e8b3e0ceb7405fa6c0940730f60804",
      "0c381c9ceeb04677903611735c1b7fcf",
      "90fc7f82fa7c4fe79386e766fe687370",
      "02035c4cf9f142c6a7167d4d81befce0",
      "0ef9969aa23348e39e56981b0ce9ba35",
      "ba9d04c9c86e45f9a80f3a61ee220c5e",
      "317a758830264800bbd6f17ee10d5e22",
      "a54841dde9cd42db85b326faaf4ab0b5",
      "72c8076ed09748e1a37b21f25d12dafe",
      "b78234a537214afebcfdee5f32d4d9a0",
      "4090d3b6cdbf45e9b54f65e55a994713",
      "41bb03671ae14450aaeb13b9ee9a9ffa",
      "0825bfd7305b497abaac3d5edbcfb72d",
      "03c8fbe4a1ab403dae44d77ff0240d9a",
      "204a396320264c7fa63b2e539f1b25d1",
      "6a2f31e069e54cfb96dfbb5b80e5d15b",
      "45dc666b5ac94bbf9164058d0c7cd3e2",
      "07718c8acd3049c4a38778e6f8d4eca3",
      "e053ba80dc79470d8b1d1c9dc67db792",
      "bab624c7fda8476d974253c9fba8a827"
     ]
    },
    "id": "YxK0emCj_0ad",
    "outputId": "f823051a-2dab-49df-b472-5373cde384a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d44dbb09423434b80e072d47b8e7a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01775f493f6f4f3cb0bd756e253cb424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d06d7f7fd240079697c7de0321be91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015c8d2f72ce4cf784b01ba33cf0cefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20386686ed4f4ddbbacc35a183418cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Iteration 1: GNN Loss = 1.0962, LLM Loss = 0.8473\n",
      "  GNN predictions: [0, 0, 1]\n",
      "  LLM predictions: [0, 0, 0]\n",
      "Iteration 2: GNN Loss = 1.1164, LLM Loss = 1.1368\n",
      "  GNN predictions: [1, 1, 1]\n",
      "  LLM predictions: [0, 0, 0]\n",
      "Iteration 3: GNN Loss = 0.8742, LLM Loss = 0.6527\n",
      "  GNN predictions: [0, 0, 0]\n",
      "  LLM predictions: [0, 0, 0]\n",
      "Iteration 4: GNN Loss = 0.5780, LLM Loss = 0.6721\n",
      "  GNN predictions: [0, 0, 0]\n",
      "  LLM predictions: [0, 0, 0]\n",
      "Iteration 5: GNN Loss = 0.4746, LLM Loss = 0.6496\n",
      "  GNN predictions: [0, 0, 0]\n",
      "  LLM predictions: [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Define the Graph Neural Network (GNN)\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)  # Output num_classes\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x  # Return logits (not softmax)\n",
    "\n",
    "# 2. Define the Text Encoder (BERT-based)\n",
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=3):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        # Project from BERT's hidden size to number of classes\n",
    "        self.classifier = torch.nn.Linear(self.model.config.hidden_size, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        # Tokenize and encode text data\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():  # Freeze BERT parameters during training\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Use [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_embedding = self.dropout(cls_embedding)\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        return logits  # Return logits (not softmax)\n",
    "\n",
    "\n",
    "# 4. Training Loop with Bidirectional Pseudo-label Exchange\n",
    "def train_prediction_alignment(data, gnn, text_encoder, num_iterations=5):\n",
    "    optimizer_gnn = torch.optim.Adam(gnn.parameters(), lr=0.01)\n",
    "    optimizer_text = torch.optim.Adam(text_encoder.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Initialize with true labels for first iteration\n",
    "    gnn_pseudo_labels = data.y.clone()\n",
    "    llm_pseudo_labels = data.y.clone()\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # 4.1 Train GNN using LLM pseudo-labels from previous iteration\n",
    "        gnn.train()\n",
    "        optimizer_gnn.zero_grad()\n",
    "        gnn_logits = gnn(data.x, data.edge_index)\n",
    "        gnn_loss = torch.nn.CrossEntropyLoss()(gnn_logits, llm_pseudo_labels)\n",
    "        gnn_loss.backward()\n",
    "        optimizer_gnn.step()\n",
    "        \n",
    "        # Generate new GNN pseudo-labels\n",
    "        with torch.no_grad():\n",
    "            gnn_pseudo_labels = torch.argmax(gnn_logits, dim=1)\n",
    "        \n",
    "        # 4.2 Train Text Encoder using GNN pseudo-labels\n",
    "        text_encoder.train()\n",
    "        optimizer_text.zero_grad()\n",
    "        text_logits = text_encoder(data.text)\n",
    "        llm_loss = torch.nn.CrossEntropyLoss()(text_logits, gnn_pseudo_labels)\n",
    "        llm_loss.backward()\n",
    "        optimizer_text.step()\n",
    "        \n",
    "        # Generate new LLM pseudo-labels for next iteration\n",
    "        with torch.no_grad():\n",
    "            llm_pseudo_labels = torch.argmax(text_logits, dim=1)\n",
    "        \n",
    "        print(f\"Iteration {iteration+1}: GNN Loss = {gnn_loss.item():.4f}, LLM Loss = {llm_loss.item():.4f}\")\n",
    "        print(f\"  GNN predictions: {gnn_pseudo_labels.tolist()}\")\n",
    "        print(f\"  LLM predictions: {llm_pseudo_labels.tolist()}\")\n",
    "\n",
    "# Initialize models and train\n",
    "input_dim = data.x.size(1)  # Node feature dimension\n",
    "hidden_dim = 64\n",
    "\n",
    "gnn = GNN(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "text_encoder = TextEncoder(num_classes=num_classes)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_prediction_alignment(data, gnn, text_encoder, num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwmGzCmH_669",
    "outputId": "2577a628-8f1c-4efb-c732-1dfbbade06bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.0187177658081055\n",
      "Epoch 2: Loss = 0.684087336063385\n",
      "Epoch 3: Loss = 1.0262978076934814\n",
      "Epoch 4: Loss = 1.098049521446228\n",
      "Epoch 5: Loss = 1.0942996740341187\n",
      "Epoch 6: Loss = 1.092896580696106\n",
      "Epoch 7: Loss = 1.0904731750488281\n",
      "Epoch 8: Loss = 1.0973970890045166\n",
      "Epoch 9: Loss = 1.0980571508407593\n",
      "Epoch 10: Loss = 1.0973337888717651\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 1. Define the GNN\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv = GraphConv(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv(x, edge_index)\n",
    "\n",
    "# 2. Define the Text Encoder (LLM)\n",
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", output_dim=128):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = torch.nn.Linear(self.model.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
    "        return self.fc(cls_embedding)\n",
    "\n",
    "# 3. Contrastive Learning Objective\n",
    "def contrastive_loss(graph_emb, text_emb, tau=0.1):\n",
    "    sim = F.cosine_similarity(graph_emb.unsqueeze(1), text_emb.unsqueeze(0), dim=2)\n",
    "    labels = torch.arange(sim.size(0)).to(sim.device)\n",
    "    loss = F.cross_entropy(sim / tau, labels)\n",
    "    return loss\n",
    "\n",
    "# 4. Training Loop for Latent Space Alignment\n",
    "def train_latent_alignment(data, gnn, text_encoder, epochs=10):\n",
    "    optimizer = torch.optim.Adam(list(gnn.parameters()) + list(text_encoder.parameters()), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encode graph and text\n",
    "        graph_emb = gnn(data.x, data.edge_index)  # Graph embeddings\n",
    "        text_emb = text_encoder(data.text)  # Text embeddings\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        loss = contrastive_loss(graph_emb, text_emb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")\n",
    "\n",
    "# 5. Example Data\n",
    "# Toy data with 3 products and their relationships\n",
    "data = Data(\n",
    "    x=torch.rand(3, 10),  # Node features\n",
    "    edge_index=torch.tensor([[0, 1], [1, 2]], dtype=torch.long).t().contiguous(),  # Edges (transposed for PyG)\n",
    "    text=[\"Product A description\", \"Product B description\", \"Product C description\"],  # Text data\n",
    ")\n",
    "\n",
    "# Initialize models and train\n",
    "gnn = GNN(input_dim=10, hidden_dim=128)\n",
    "text_encoder = TextEncoder()\n",
    "train_latent_alignment(data, gnn, text_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqcZZn6qCCmj"
   },
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM73g-bDiPPG"
   },
   "source": [
    "If using Colab you can simply run the following cells.\n",
    "\n",
    "Otherwise, if you want to use the local backend, please:\n",
    "- download neo4j desktop on [docker](https://neo4j.com/docs/graph-data-science/current/installation/installation-docker/)*\n",
    "- download [lm-studio](https://lmstudio.ai/) and download the minicpm-llama3-v-2_5 and nomic-embed-text model\n",
    "\n",
    "*run docker as:\n",
    "\n",
    "\n",
    "```\n",
    "docker run --rm --env NEO4J_AUTH=neo4j/defaultpass -p 7474:7474 -p 7687:7687 -v $PWD/data:/data -v $PWD/plugins:/plugins --name neo4j-apoc -e NEO4J_apoc_export_file_enabled=true -e NEO4J_apoc_import_file_enabled=true -e NEO4J_apoc_import_file_use__neo4j__config=true -e NEO4J_PLUGINS=\\[\\\"apoc-extended\\\"\\] neo4j\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKEORJfWwI7w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "LLM_BACKEND = \"ollama\" # choose [\"ollama\" | \"lm-studio\"]\n",
    "# LLM_BACKEND = \"lm-studio\"\n",
    "\n",
    "assert LLM_BACKEND in [\"ollama\", \"lm-studio\"]\n",
    "\n",
    "if LLM_BACKEND == \"ollama\":\n",
    "  base_url = f\"http://{os.environ.get('OLLAMA_HOST', 'localhost')}:11434/v1\"\n",
    "  api_key = \"ollama\"\n",
    "  \n",
    "  # Model selection - llama3.2 is MUCH faster than phi4\n",
    "  llm_model = \"llama3.2\"  # Fast and efficient (3B parameters)\n",
    "  # llm_model = \"phi4\"    # Slower but more capable (14B parameters)\n",
    "  # llm_model = \"gemma2\"  # Alternative fast option\n",
    "else:\n",
    "  base_url = \"http://localhost:1234/v1\"\n",
    "  api_key = \"lm-studio\"\n",
    "  llm_model = \"minicpm-llama3-v-2_5\"\n",
    "\n",
    "print(f\"Using model: {llm_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIEW2lBMqp2V"
   },
   "source": [
    "If Colab you need to download ollama and start the server - run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Ollama in Colab...\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "Starting Ollama server...\n",
      "Ollama server started!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Installing Ollama in Colab...\")\n",
    "    # Download and install Ollama\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    \n",
    "    # Start Ollama server in the background\n",
    "    print(\"Starting Ollama server...\")\n",
    "    process = subprocess.Popen(['ollama', 'serve'], \n",
    "                               stdout=subprocess.PIPE, \n",
    "                               stderr=subprocess.PIPE)\n",
    "    \n",
    "    # Wait for server to start\n",
    "    time.sleep(5)\n",
    "    print(\"Ollama server started!\")\n",
    "else:\n",
    "    print(\"Not in Colab. Please ensure Ollama is installed and running:\")\n",
    "    print(\"- Download from: https://ollama.com/download\")\n",
    "    print(\"- Make sure 'ollama serve' is running in a terminal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Ollama status...\n",
      "\n",
      "‚úì Ollama server is running and accessible!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Ollama connection and start if needed\n",
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"Checking Ollama status...\\n\")\n",
    "\n",
    "# Check if Ollama is accessible\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434', timeout=2)\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úì Ollama server is running and accessible!\")\n",
    "    else:\n",
    "        print(f\"‚ö† Ollama responded with status: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚úó Ollama server is NOT accessible!\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîß FIXING: Starting Ollama server...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Try to start Ollama in the background\n",
    "        print(\"\\nStarting 'ollama serve' in background...\")\n",
    "        process = subprocess.Popen(\n",
    "            ['ollama', 'serve'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            start_new_session=True\n",
    "        )\n",
    "        \n",
    "        # Wait a moment for server to start\n",
    "        print(\"Waiting for Ollama to start...\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Check again\n",
    "        try:\n",
    "            response = requests.get('http://localhost:11434', timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                print(\"‚úì Ollama server started successfully!\\n\")\n",
    "            else:\n",
    "                print(f\"‚ö† Ollama started but responded with: {response.status_code}\\n\")\n",
    "        except:\n",
    "            print(\"‚ö† Ollama may still be starting up...\\n\")\n",
    "            print(\"If issues persist, manually run in terminal:\")\n",
    "            print(\"  ollama serve\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n‚úó Ollama is not installed!\")\n",
    "        print(\"\\nüì• Install Ollama:\")\n",
    "        print(\"  Visit: https://ollama.com/download\")\n",
    "        print(\"  Or run the Colab setup cell above if in Colab\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error starting Ollama: {e}\")\n",
    "        print(\"\\nManually start Ollama:\")\n",
    "        print(\"  Open a new terminal and run: ollama serve\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error checking Ollama: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.11.10)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Preparing models for LLM extraction...\n",
      "\n",
      "1. Checking Ollama connectivity...\n",
      "   ‚úì Ollama is accessible\n",
      "\n",
      "   Found 0 models: []\n",
      "\n",
      "2. Downloading phi4 model...\n",
      "   ‚è≥ This may take a few minutes (phi4 is ~8GB)...\n",
      "   Please be patient...\n",
      "\n",
      "   ‚úì phi4 downloaded in 0.4 seconds\n",
      "\n",
      "3. Checking embedding model...\n",
      "   Downloading nomic-embed-text...\n",
      "   ‚úì nomic-embed-text downloaded\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All models ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "print(\"Preparing models for LLM extraction...\\n\")\n",
    "\n",
    "# First, verify Ollama is responding\n",
    "try:\n",
    "    print(\"1. Checking Ollama connectivity...\")\n",
    "    models_response = ollama.list()\n",
    "    print(\"   ‚úì Ollama is accessible\\n\")\n",
    "    \n",
    "    # Check if phi4 is already available\n",
    "    # Handle different response formats from ollama.list()\n",
    "    model_names = []\n",
    "    if isinstance(models_response, dict):\n",
    "        if 'models' in models_response:\n",
    "            for m in models_response['models']:\n",
    "                if isinstance(m, dict) and 'name' in m:\n",
    "                    model_names.append(m['name'])\n",
    "                elif isinstance(m, dict) and 'model' in m:\n",
    "                    model_names.append(m['model'])\n",
    "    \n",
    "    print(f\"   Found {len(model_names)} models: {model_names}\\n\")\n",
    "    \n",
    "    # Check if the selected model is available\n",
    "    model_available = any(llm_model in str(name).lower() for name in model_names)\n",
    "    \n",
    "    if model_available:\n",
    "        print(\"2. Model check:\")\n",
    "        print(f\"   ‚úì {llm_model} is already available!\\n\")\n",
    "    else:\n",
    "        print(f\"2. Downloading {llm_model} model...\")\n",
    "        if llm_model == \"llama3.2\":\n",
    "            print(\"   ‚è≥ This should be quick (~2GB download)...\")\n",
    "        elif llm_model == \"phi4\":\n",
    "            print(\"   ‚è≥ This may take a few minutes (~8GB download)...\")\n",
    "        else:\n",
    "            print(\"   ‚è≥ Downloading...\")\n",
    "        print(\"   Please be patient...\\n\")\n",
    "        \n",
    "        start = time.time()\n",
    "        ollama.pull(llm_model)\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"   ‚úì {llm_model} downloaded in {elapsed:.1f} seconds\\n\")\n",
    "    \n",
    "    # Optionally pull embedding model\n",
    "    print(\"3. Checking embedding model...\")\n",
    "    nomic_available = any('nomic-embed-text' in str(name).lower() for name in model_names)\n",
    "    \n",
    "    if nomic_available:\n",
    "        print(\"   ‚úì nomic-embed-text is already available\\n\")\n",
    "    else:\n",
    "        print(\"   Downloading nomic-embed-text...\")\n",
    "        ollama.pull(\"nomic-embed-text\")\n",
    "        print(\"   ‚úì nomic-embed-text downloaded\\n\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"‚úÖ All models ready!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {e}\\n\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"üîß TROUBLESHOOTING:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n1. Is Ollama running?\")\n",
    "    print(\"   ‚Üí Run Cell 11 to start it automatically\")\n",
    "    print(\"   ‚Üí OR manually run in terminal: ollama serve\\n\")\n",
    "    print(\"2. Is Ollama installed?\")\n",
    "    print(\"   ‚Üí Download from: https://ollama.com/download\\n\")\n",
    "    print(\"3. Is the model stuck downloading?\")\n",
    "    print(\"   ‚Üí Check terminal where 'ollama serve' is running\")\n",
    "    print(\"   ‚Üí Try: ollama pull phi4 (in a separate terminal)\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up the Model\n",
    "\n",
    "**Important:** The first time a model loads, it needs to be loaded into memory. This cell pre-loads it:\n",
    "\n",
    "- **llama3.2** (current): ~10-20 seconds  \n",
    "- **phi4**: ~30-60 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading phi4 model into memory...\n",
      "‚è≥ First load may take 30-60 seconds, please be patient...\n",
      "\n",
      "‚úì Model loaded successfully in 57.4 seconds!\n",
      "Model response: Hello! How can I...\n",
      "\n",
      "======================================================================\n",
      "‚úÖ phi4 is ready to use!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Warm up the model - loads it into memory\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "print(f\"Loading {llm_model} model into memory...\")\n",
    "if llm_model == \"llama3.2\":\n",
    "    print(\"‚è≥ This should be quick (10-20 seconds)...\\n\")\n",
    "elif llm_model == \"phi4\":\n",
    "    print(\"‚è≥ First load may take 30-60 seconds, please be patient...\\n\")\n",
    "else:\n",
    "    print(\"‚è≥ Loading model, please be patient...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Use ollama.generate which is more reliable than chat for testing\n",
    "    response = ollama.generate(\n",
    "        model=llm_model,\n",
    "        prompt='Hello',\n",
    "        options={'num_predict': 5}  # Only generate 5 tokens to be fast\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úì Model loaded successfully in {elapsed:.1f} seconds!\")\n",
    "    print(f\"Model response: {response['response'][:50]}...\\n\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚úÖ {llm_model} is ready to use!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úó Model failed to load after {elapsed:.1f} seconds\")\n",
    "    print(f\"Error: {e}\\n\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üîß TROUBLESHOOTING:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n1. Check if model is downloaded:\")\n",
    "    print(\"   ‚Üí Run: ollama list (in terminal)\")\n",
    "    print(f\"   ‚Üí If {llm_model} is missing, run: ollama pull {llm_model}\\n\")\n",
    "    print(\"2. Model might be corrupted:\")\n",
    "    print(f\"   ‚Üí Run: ollama rm {llm_model}\")\n",
    "    print(f\"   ‚Üí Then: ollama pull {llm_model}\\n\")\n",
    "    print(\"3. Try the model directly:\")\n",
    "    print(f\"   ‚Üí In terminal, run: ollama run {llm_model}\")\n",
    "    print(\"   ‚Üí Type 'hello' and see if it responds\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Steps to Use Option A (LLM Extraction)\n",
    "\n",
    "Since you want to use LLM extraction, follow these steps in order:\n",
    "\n",
    "1. **Cell 8**: Model selection ‚úì (now using llama3.2 - much faster!)\n",
    "2. **Cell 11**: Check/Start Ollama ‚úì  \n",
    "3. **Cell 12**: Download llama3.2 model (if not already downloaded)\n",
    "4. **Cell 14**: üëà **START HERE** - Warm up the model (~10-20 seconds)\n",
    "5. **Cell 20**: Test LLM connection  \n",
    "6. **Cell 26**: Run Option A extraction (1-3 minutes with llama3.2)\n",
    "\n",
    "**Current model:** llama3.2 (3B parameters, fast and efficient)\n",
    "\n",
    "**Want more accuracy?** Change Cell 8 to use phi4, but expect longer wait times (2-5 min for extraction)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Neo4j in Colab...\n",
      "Requirement already satisfied: neo4j in /usr/local/lib/python3.12/dist-packages (5.28.2)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from neo4j) (2025.2)\n",
      "\n",
      "‚ö†Ô∏è  Neo4j Setup Required:\n",
      "For Colab, you have two options:\n",
      "1. Use Neo4j AuraDB (free cloud instance): https://neo4j.com/cloud/aura/\n",
      "2. Use a temporary Neo4j sandbox: https://sandbox.neo4j.com/\n",
      "\n",
      "Then update NEO4J_URI, NEO4J_USER, and NEO4J_PASSWORD in the next cell\n"
     ]
    }
   ],
   "source": [
    "# Setup Neo4j for Colab or local environment\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Setting up Neo4j in Colab...\")\n",
    "    # Install Neo4j driver\n",
    "    %pip install neo4j\n",
    "    \n",
    "    # Note: For full Neo4j in Colab, you'd typically use a cloud instance\n",
    "    # or a temporary docker container. For simplicity, we'll show instructions\n",
    "    print(\"\\n‚ö†Ô∏è  Neo4j Setup Required:\")\n",
    "    print(\"For Colab, you have two options:\")\n",
    "    print(\"1. Use Neo4j AuraDB (free cloud instance): https://neo4j.com/cloud/aura/\")\n",
    "    print(\"2. Use a temporary Neo4j sandbox: https://sandbox.neo4j.com/\")\n",
    "    print(\"\\nThen update NEO4J_URI, NEO4J_USER, and NEO4J_PASSWORD in the next cell\")\n",
    "else:\n",
    "    print(\"Local environment detected.\")\n",
    "    print(\"\\nüìã Neo4j Setup Instructions:\")\n",
    "    print(\"\\nOption 1: Using Docker (Recommended):\")\n",
    "    print(\"Run this command in your terminal:\")\n",
    "    print('docker run --rm --env NEO4J_AUTH=neo4j/defaultpass -p 7474:7474 -p 7687:7687 --name neo4j neo4j:latest')\n",
    "    print(\"\\nOption 2: Download Neo4j Desktop:\")\n",
    "    print(\"https://neo4j.com/download/\")\n",
    "    print(\"\\nOption 3: Use Neo4j AuraDB (Cloud, free tier):\")\n",
    "    print(\"https://neo4j.com/cloud/aura/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úó Cannot connect to Neo4j on localhost:7687\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  IMPORTANT: Neo4j is NOT running - you must set it up first!\n",
      "======================================================================\n",
      "\n",
      "üåê COLAB USERS: You cannot run Neo4j locally in Colab.\n",
      "\n",
      "You MUST use a cloud Neo4j instance. Choose ONE option:\n",
      "\n",
      "Option 1: Neo4j AuraDB (FREE, Recommended)\n",
      "  1. Go to: https://neo4j.com/cloud/aura/\n",
      "  2. Sign up for free account\n",
      "  3. Create a new FREE instance\n",
      "  4. Save the credentials they give you\n",
      "  5. In the next cell, update:\n",
      "     NEO4J_URI = 'neo4j+s://xxxxx.databases.neo4j.io'\n",
      "     NEO4J_USER = 'neo4j'\n",
      "     NEO4J_PASSWORD = 'your-password-from-aura'\n",
      "\n",
      "Option 2: Neo4j Sandbox (Temporary)\n",
      "  1. Go to: https://sandbox.neo4j.com/\n",
      "  2. Create a blank sandbox\n",
      "  3. Get connection details\n",
      "  4. Update connection info in next cell\n",
      "\n",
      "‚ö†Ô∏è  DO NOT run the next cell until Neo4j is set up and this cell shows ‚úì\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Neo4j connection\n",
    "import socket\n",
    "import os\n",
    "\n",
    "def test_neo4j_connection(host='localhost', port=7687):\n",
    "    \"\"\"Test if Neo4j is accessible on the given host and port\"\"\"\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(2)\n",
    "        result = sock.connect_ex((host, port))\n",
    "        sock.close()\n",
    "        return result == 0\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "host = os.environ.get(\"NEO4J_HOST\", \"localhost\")\n",
    "\n",
    "if test_neo4j_connection(host):\n",
    "    print(f\"‚úì Neo4j is accessible on {host}:7687\")\n",
    "    print(\"You can proceed to the next cell!\")\n",
    "else:\n",
    "    print(f\"‚úó Cannot connect to Neo4j on {host}:7687\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  IMPORTANT: Neo4j is NOT running - you must set it up first!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        print(\"\\nüåê COLAB USERS: You cannot run Neo4j locally in Colab.\")\n",
    "        print(\"\\nYou MUST use a cloud Neo4j instance. Choose ONE option:\\n\")\n",
    "        print(\"Option 1: Neo4j AuraDB (FREE, Recommended)\")\n",
    "        print(\"  1. Go to: https://neo4j.com/cloud/aura/\")\n",
    "        print(\"  2. Sign up for free account\")\n",
    "        print(\"  3. Create a new FREE instance\")\n",
    "        print(\"  4. Save the credentials they give you\")\n",
    "        print(\"  5. In the next cell, update:\")\n",
    "        print(\"     NEO4J_URI = 'neo4j+s://xxxxx.databases.neo4j.io'\")\n",
    "        print(\"     NEO4J_USER = 'neo4j'\")\n",
    "        print(\"     NEO4J_PASSWORD = 'your-password-from-aura'\")\n",
    "        print(\"\\nOption 2: Neo4j Sandbox (Temporary)\")\n",
    "        print(\"  1. Go to: https://sandbox.neo4j.com/\")\n",
    "        print(\"  2. Create a blank sandbox\")\n",
    "        print(\"  3. Get connection details\")\n",
    "        print(\"  4. Update connection info in next cell\")\n",
    "    else:\n",
    "        print(\"\\nüñ•Ô∏è  LOCAL USERS: Start Neo4j with Docker\\n\")\n",
    "        print(\"Run this in your terminal:\")\n",
    "        print(\"‚îÄ\" * 70)\n",
    "        print(\"docker run --rm --env NEO4J_AUTH=neo4j/defaultpass \\\\\")\n",
    "        print(\"  -p 7474:7474 -p 7687:7687 --name neo4j neo4j:latest\")\n",
    "        print(\"‚îÄ\" * 70)\n",
    "        print(\"\\nOR download Neo4j Desktop: https://neo4j.com/download/\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  DO NOT run the next cell until Neo4j is set up and this cell shows ‚úì\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='nomic-embed-text:latest', modified_at=datetime.datetime(2025, 11, 18, 17, 51, 28, 820938, tzinfo=TzInfo(UTC)), digest='0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f', size=274302450, details=ModelDetails(parent_model='', format='gguf', family='nomic-bert', families=['nomic-bert'], parameter_size='137M', quantization_level='F16')), Model(model='phi4:latest', modified_at=datetime.datetime(2025, 11, 18, 17, 51, 28, 594939, tzinfo=TzInfo(UTC)), digest='ac896e5b8b34a1f4efa7b14d7520725140d5512484457fab45d2a4ea14c69dba', size=9053116391, details=ModelDetails(parent_model='', format='gguf', family='phi3', families=['phi3'], parameter_size='14.7B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Section\n",
    "\n",
    "> **üìù Note:** The GraphRAG section requires a running Neo4j database. \n",
    ">\n",
    "> **For Colab users:** You'll need to set up a free Neo4j AuraDB instance (instructions below).\n",
    ">\n",
    "> **Want to skip this section?** The first two sections (Prediction Alignment & Latent Space Alignment) work without Neo4j and demonstrate core LLM+Graph concepts!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK17PDF7KxCv"
   },
   "source": [
    "# Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configure Neo4j Connection\n",
    "\n",
    "**IMPORTANT:** Update these settings based on your Neo4j setup:\n",
    "\n",
    "### For Local Docker (default settings):\n",
    "- Keep the defaults below (no changes needed)\n",
    "\n",
    "### For Neo4j AuraDB (Colab users):\n",
    "- Update `NEO4J_URI` to your AuraDB URI (e.g., `neo4j+s://xxxxx.databases.neo4j.io`)\n",
    "- Update `NEO4J_PASSWORD` to your AuraDB password\n",
    "- Keep `NEO4J_USER` as `neo4j`\n",
    "\n",
    "### For Neo4j Sandbox:\n",
    "- Update all three variables with your sandbox credentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Quick Test: Check LLM Connection\n",
    "\n",
    "Before running the full graph extraction (which can be slow), let's test if the LLM is responding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM connection...\n",
      "Backend: ollama\n",
      "Model: phi4\n",
      "Base URL: http://localhost:11434/v1\n",
      "\n",
      "1. Checking if Ollama server is accessible...\n",
      "   ‚úì Ollama server is running\n",
      "\n",
      "2. Testing if model responds via LangChain...\n",
      "   (This uses the same interface as the graph extraction)\n",
      "\n",
      "   Sending test query...\n",
      "   ‚úì Model is responding!\n",
      "   Response: Hi!\n",
      "\n",
      "======================================================================\n",
      "‚úÖ LLM is working - you can proceed with Option A (LLM extraction)\n",
      "======================================================================\n",
      "\n",
      "Note: The graph extraction in Cell 22 may still take 2-5 minutes\n",
      "because it needs to analyze longer text and make multiple LLM calls.\n"
     ]
    }
   ],
   "source": [
    "# Quick test of LLM connection\n",
    "import requests\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "print(\"Testing LLM connection...\")\n",
    "print(f\"Backend: {LLM_BACKEND}\")\n",
    "print(f\"Model: {llm_model}\")\n",
    "print(f\"Base URL: {base_url}\\n\")\n",
    "\n",
    "# First check if Ollama server is accessible\n",
    "try:\n",
    "    print(\"1. Checking if Ollama server is accessible...\")\n",
    "    response = requests.get('http://localhost:11434', timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"   ‚úì Ollama server is running\\n\")\n",
    "    else:\n",
    "        print(f\"   ‚ö† Ollama server responded with status: {response.status_code}\\n\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"   ‚úó Ollama server is NOT running!\")\n",
    "    print(\"   ‚Üí Start Ollama: Open terminal and run 'ollama serve'\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {e}\\n\")\n",
    "\n",
    "# Then test if the model responds via LangChain\n",
    "try:\n",
    "    print(\"2. Testing if model responds via LangChain...\")\n",
    "    print(\"   (This uses the same interface as the graph extraction)\\n\")\n",
    "    \n",
    "    test_llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model_name=llm_model,\n",
    "        base_url=base_url,\n",
    "        api_key=api_key,\n",
    "        timeout=60,  # 60 second timeout for first load\n",
    "        max_retries=1\n",
    "    )\n",
    "    \n",
    "    print(\"   Sending test query...\")\n",
    "    response = test_llm.invoke(\"Say 'hello' in one word\")\n",
    "    print(f\"   ‚úì Model is responding!\")\n",
    "    print(f\"   Response: {response.content}\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ LLM is working - you can proceed with Option A (LLM extraction)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nNote: The graph extraction in Cell 22 may still take 2-5 minutes\")\n",
    "    print(\"because it needs to analyze longer text and make multiple LLM calls.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Model test failed: {e}\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  LLM is NOT working properly\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüîß Try these fixes:\")\n",
    "    print(\"1. Did you run Cell 14 to warm up the model?\")\n",
    "    print(\"   ‚Üí Go back and run Cell 14 first\")\n",
    "    print(\"2. Check if phi4 is properly installed:\")\n",
    "    print(\"   ‚Üí Open terminal and run: ollama list\")\n",
    "    print(\"   ‚Üí If phi4 is missing: ollama pull phi4\")\n",
    "    print(\"3. Try using ollama directly:\")\n",
    "    print(\"   ‚Üí Open terminal and run: ollama run phi4\")\n",
    "    print(\"   ‚Üí Type 'hello' and see if it responds\")\n",
    "    print(\"4. Consider using a faster model:\")\n",
    "    print(\"   ‚Üí In Cell 8, change: llm_model = 'llama3.2'\")\n",
    "    print(\"   ‚Üí Then: ollama pull llama3.2\")\n",
    "    print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: LLM-Based Graph Extraction (Slow)\n",
    "\n",
    "**‚ö†Ô∏è Warning:** This can take 2-5 minutes with phi4. Only run if the LLM test above succeeded.\n",
    "\n",
    "If this cell runs too long, **interrupt it** and use Option B (manual) below instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUWxS0p0-d-e",
    "outputId": "527d26fc-0dfc-479b-f1c8-330ec8b7d52b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in /usr/local/lib/python3.12/dist-packages (5.28.2)\n",
      "Requirement already satisfied: langchain-neo4j in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
      "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
      "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from neo4j) (2025.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-neo4j) (1.0.0)\n",
      "Requirement already satisfied: neo4j-graphrag<2.0.0,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from langchain-neo4j) (1.10.1)\n",
      "Requirement already satisfied: langchain-community<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-experimental) (0.4.1)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.42)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.11.10)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.32.5)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.0.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: fsspec<2025.0.0,>=2024.9.0 in /usr/local/lib/python3.12/dist-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (2024.12.0)\n",
      "Requirement already satisfied: json-repair<0.45.0,>=0.44.1 in /usr/local/lib/python3.12/dist-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (0.44.1)\n",
      "Requirement already satisfied: pypdf<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (6.3.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (1.16.3)\n",
      "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /usr/local/lib/python3.12/dist-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (6.0.12.20250915)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3.0.0,>=1.4.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.1.0)\n",
      "Attempting to connect to: neo4j+s://edea191c.databases.neo4j.io\n",
      "User: neo4j\n",
      "\n",
      "‚úì Connected to Neo4j at neo4j+s://edea191c.databases.neo4j.io\n",
      "‚úì Neo4j Graph initialized successfully\n",
      "\n",
      "Converting text to graph documents...\n",
      "‚è≥ This may take 30-90 seconds as the LLM extracts entities and relationships...\n",
      "   The LLM is analyzing the text to identify nodes (entities) and edges (relationships)\n",
      "   Please be patient...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mget_executor_for_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    570\u001b[0m     ) as executor:\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3853\u001b[0m                 output = {\n\u001b[0;32m-> 3854\u001b[0;31m                     \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3855\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2767055147.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgraph_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_graph_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úì Conversion completed in {elapsed:.1f} seconds\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36mconvert_to_graph_documents\u001b[0;34m(self, documents, config)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \"\"\"\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     async def aprocess_response(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_experimental/graph_transformers/llm.py\u001b[0m in \u001b[0;36mprocess_response\u001b[0;34m(self, document, config)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m    838\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mraw_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mraw_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3128\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3130\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mget_executor_for_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m                 futures = [\n\u001b[1;32m   3850\u001b[0m                     \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_invoke_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mget_executor_for_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \"\"\"\n\u001b[1;32m    567\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m     with ContextThreadPoolExecutor(\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_concurrency\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     ) as executor:\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%pip install neo4j langchain-neo4j langchain-experimental langchain-openai langchain-core\n",
    "\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üîß CONFIGURE YOUR NEO4J CONNECTION HERE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# For LOCAL Docker (default):\n",
    "host = os.environ.get(\"NEO4J_HOST\", \"localhost\")\n",
    "NEO4J_URI = \"neo4j+s://edea191c.databases.neo4j.io\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"YMlZaZmpSVwzkjKdok57WI7PoNO-4YvVVfsLNvH3Ud4\"\n",
    "\n",
    "# For COLAB/AuraDB: Uncomment and update these lines:\n",
    "# NEO4J_URI = \"neo4j+s://xxxxx.databases.neo4j.io\"  # Your AuraDB URI\n",
    "# NEO4J_USER = \"neo4j\"\n",
    "# NEO4J_PASSWORD = \"your-auradb-password\"  # Your AuraDB password\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(f\"Attempting to connect to: {NEO4J_URI}\")\n",
    "print(f\"User: {NEO4J_USER}\")\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    driver.verify_connectivity()\n",
    "    print(f\"\\n‚úì Connected to Neo4j at {NEO4J_URI}\")\n",
    "    \n",
    "    graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASSWORD)\n",
    "    print(\"‚úì Neo4j Graph initialized successfully\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Failed to connect to Neo4j\")\n",
    "    print(f\"Error: {e}\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîß TROUBLESHOOTING STEPS:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n1. Did you run Cell 14 to check if Neo4j is accessible?\")\n",
    "    print(\"   ‚Üí Go back and run Cell 14 first!\\n\")\n",
    "    print(\"2. Are you in Colab?\")\n",
    "    print(\"   ‚Üí You MUST use Neo4j AuraDB (free): https://neo4j.com/cloud/aura/\")\n",
    "    print(\"   ‚Üí Then update the connection settings in this cell (lines 11-14)\\n\")\n",
    "    print(\"3. Are you running locally?\")\n",
    "    print(\"   ‚Üí Start Neo4j with Docker in a terminal:\")\n",
    "    print(\"   docker run --rm --env NEO4J_AUTH=neo4j/defaultpass \\\\\")\n",
    "    print(\"     -p 7474:7474 -p 7687:7687 --name neo4j neo4j:latest\\n\")\n",
    "    print(\"4. Using AuraDB but still failing?\")\n",
    "    print(\"   ‚Üí Check your URI format: neo4j+s://xxxxx.databases.neo4j.io\")\n",
    "    print(\"   ‚Üí Check your password is correct\")\n",
    "    print(\"=\" * 70)\n",
    "    raise\n",
    "\n",
    "# ---- Step 2: Create knowledge graph from text ----\n",
    "import os\n",
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name=llm_model,\n",
    "                 base_url=base_url,\n",
    "                 api_key=api_key)\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text = \"\"\"\n",
    "Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
    "She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
    "Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
    "She was, in 1906, the first woman to become a professor at the University of Paris.\n",
    "\"\"\"\n",
    "documents = [Document(page_content=text)]\n",
    "\n",
    "print(\"Converting text to graph documents...\")\n",
    "print(\"‚è≥ This may take 30-90 seconds as the LLM extracts entities and relationships...\")\n",
    "print(\"   The LLM is analyzing the text to identify nodes (entities) and edges (relationships)\")\n",
    "print(\"   Please be patient...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úì Conversion completed in {elapsed:.1f} seconds\\n\")\n",
    "    \n",
    "    if graph_documents and len(graph_documents) > 0:\n",
    "        print(f\"üìä Extracted {len(graph_documents[0].nodes)} Nodes:\")\n",
    "        for i, node in enumerate(graph_documents[0].nodes[:5], 1):  # Show first 5\n",
    "            print(f\"  {i}. {node}\")\n",
    "        if len(graph_documents[0].nodes) > 5:\n",
    "            print(f\"  ... and {len(graph_documents[0].nodes) - 5} more\\n\")\n",
    "        else:\n",
    "            print()\n",
    "        \n",
    "        print(f\"üîó Extracted {len(graph_documents[0].relationships)} Relationships:\")\n",
    "        for i, rel in enumerate(graph_documents[0].relationships[:5], 1):  # Show first 5\n",
    "            print(f\"  {i}. {rel}\")\n",
    "        if len(graph_documents[0].relationships) > 5:\n",
    "            print(f\"  ... and {len(graph_documents[0].relationships) - 5} more\\n\")\n",
    "        else:\n",
    "            print()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No graph documents were created\")\n",
    "        \n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úó Conversion failed after {elapsed:.1f} seconds\")\n",
    "    print(f\"Error: {e}\\n\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"1. Ollama/phi4 not responding properly\")\n",
    "    print(\"2. Model timeout or memory issues\")\n",
    "    print(\"3. LLM unable to parse the text\")\n",
    "    raise\n",
    "\n",
    "# Add graph to neo4j\n",
    "print(\"\\nAdding graph documents to Neo4j...\")\n",
    "try:\n",
    "    graph.add_graph_documents(graph_documents)\n",
    "    print(\"‚úì Graph documents added successfully\")\n",
    "    print(\"\\nüëâ Now skip to 'Step 3: Perform GraphRAG Queries' below\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to add graph documents: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Manual Graph Creation (Fast - Recommended) ‚ö°\n",
    "\n",
    "**‚úÖ USE THIS if:**\n",
    "- LLM test failed above\n",
    "- Option A is too slow\n",
    "- You want instant results\n",
    "\n",
    "**No LLM needed!** This creates the graph directly - works even if Ollama isn't running!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GraphDocument' from 'langchain_experimental.graph_transformers' (/usr/local/lib/python3.12/dist-packages/langchain_experimental/graph_transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4053591009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Manual graph creation (bypasses slow LLM extraction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_experimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_document\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRelationship\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GraphDocument' from 'langchain_experimental.graph_transformers' (/usr/local/lib/python3.12/dist-packages/langchain_experimental/graph_transformers/__init__.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Manual graph creation (bypasses slow LLM extraction)\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import GraphDocument\n",
    "from langchain_community.graphs.graph_document import Node, Relationship\n",
    "\n",
    "print(\"Creating graph manually...\")\n",
    "\n",
    "# Create nodes\n",
    "marie = Node(id=\"Marie Curie\", type=\"Person\")\n",
    "pierre = Node(id=\"Pierre Curie\", type=\"Person\")\n",
    "nobel = Node(id=\"Nobel Prize\", type=\"Award\")\n",
    "university = Node(id=\"University of Paris\", type=\"Organization\")\n",
    "radioactivity = Node(id=\"Radioactivity\", type=\"ResearchField\")\n",
    "\n",
    "# Create relationships\n",
    "relationships = [\n",
    "    Relationship(source=marie, target=pierre, type=\"MARRIED_TO\"),\n",
    "    Relationship(source=marie, target=nobel, type=\"WON\"),\n",
    "    Relationship(source=pierre, target=nobel, type=\"WON\"),\n",
    "    Relationship(source=marie, target=university, type=\"PROFESSOR_AT\"),\n",
    "    Relationship(source=marie, target=radioactivity, type=\"RESEARCHED\"),\n",
    "]\n",
    "\n",
    "# Create graph document\n",
    "text = \"\"\"\n",
    "Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
    "She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
    "Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
    "She was, in 1906, the first woman to become a professor at the University of Paris.\n",
    "\"\"\"\n",
    "\n",
    "graph_documents = [GraphDocument(\n",
    "    nodes=[marie, pierre, nobel, university, radioactivity],\n",
    "    relationships=relationships,\n",
    "    source=Document(page_content=text)\n",
    ")]\n",
    "\n",
    "print(f\"‚úì Created graph with {len(graph_documents[0].nodes)} nodes and {len(graph_documents[0].relationships)} relationships\\n\")\n",
    "\n",
    "print(\"üìä Nodes:\")\n",
    "for i, node in enumerate(graph_documents[0].nodes, 1):\n",
    "    print(f\"  {i}. {node.type}: {node.id}\")\n",
    "\n",
    "print(f\"\\nüîó Relationships:\")\n",
    "for i, rel in enumerate(graph_documents[0].relationships, 1):\n",
    "    print(f\"  {i}. {rel.source.id} --[{rel.type}]--> {rel.target.id}\")\n",
    "\n",
    "print(\"\\n‚úì Graph created successfully! Now adding to Neo4j...\")\n",
    "\n",
    "# Add graph to neo4j\n",
    "try:\n",
    "    graph.add_graph_documents(graph_documents)\n",
    "    print(\"‚úì Graph documents added to Neo4j successfully!\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to add graph documents: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform GraphRAG Queries\n",
    "\n",
    "Now that we have data in Neo4j, let's query it using natural language!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GraphRAG query chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "print(\"Setting up GraphRAG query chain...\")\n",
    "\n",
    "# Create LLM for queries\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=llm_model,\n",
    "    base_url=base_url,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "def escape(s):\n",
    "    return s.replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "\n",
    "# Get the current schema\n",
    "schema_info = graph.schema\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = f\"\"\"You are a Neo4j expert. Generate a Cypher query to answer the given question.\n",
    "\n",
    "Database Schema: {escape(schema_info)}\n",
    "\n",
    "Rules:\n",
    "1. Always use explicit `MATCH` for relationships.\n",
    "2. Never use `WHERE` for relationship matching.\n",
    "3. Use `RETURN DISTINCT` when appropriate.\n",
    "\n",
    "Example Queries:\n",
    "1. Question: \"Who won the Nobel Prize?\"\n",
    "   Cypher: MATCH (p:Person)-[:WON]->(:Award) RETURN p.id AS winner\n",
    "\n",
    "Question: {{query}}\n",
    "Return only the Cypher query without any explanation or additional text.\n",
    "Cypher:\"\"\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    cypher_prompt=PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=CYPHER_GENERATION_TEMPLATE\n",
    "    ),\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "print(\"‚úì GraphRAG chain ready!\\n\")\n",
    "print(\"Current database schema:\")\n",
    "print(schema_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GraphRAG queries\n",
    "print(\"Testing GraphRAG queries...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Query 1\n",
    "question1 = \"Who was married to Marie Curie?\"\n",
    "print(f\"\\nüìù Question: {question1}\")\n",
    "try:\n",
    "    response = chain.invoke(question1)\n",
    "    print(f\"‚úì Answer: {response['result']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Query 2\n",
    "question2 = \"What did Marie Curie research?\"\n",
    "print(f\"\\nüìù Question: {question2}\")\n",
    "try:\n",
    "    response = chain.invoke(question2)\n",
    "    print(f\"‚úì Answer: {response['result']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Query 3\n",
    "question3 = \"Where did Marie Curie work?\"\n",
    "print(f\"\\nüìù Question: {question3}\")\n",
    "try:\n",
    "    response = chain.invoke(question3)\n",
    "    print(f\"‚úì Answer: {response['result']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Close the driver\n",
    "print(\"\\n\\nCleaning up...\")\n",
    "try:\n",
    "    driver.close()\n",
    "    print(\"‚úì Neo4j driver closed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n‚úÖ GraphRAG demo complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4gzGQzeeqFC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
